{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "O86elXE2bj8B"
      },
      "outputs": [],
      "source": [
        "#Inspired by https://github.com/svpino/youtube-rag/blob/main/rag.ipynb\n",
        "#Installing libraries\n",
        "# %pip install --upgrade openai\n",
        "# %pip install -U openai-whisper\n",
        "# %pip install pytube\n",
        "# %pip install langchain\n",
        "# %pip install langchain_openai\n",
        "# %pip install docarray\n",
        "# %pip install --upgrade --quiet  llmlingua accelerate\n",
        "# %pip install pinecone-client==3.0.0\n",
        "# %pip install langchain_pinecone"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Chain Prompt, Model and Parser"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Go to [OpenAI](https://openai) create your own account and generate API key.\n",
        "Make sure you have copied it correctly and saved it somewhere safe.\n",
        " \n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "BLxtkSApcL7a"
      },
      "outputs": [],
      "source": [
        "#Insert openai api key and youtube video link below:\n",
        "from openai import OpenAI\n",
        "import os\n",
        "\n",
        "OPENAI_API_KEY = \"[ENTER YOUR OPENAI API KEY HERE]\"\n",
        "YOUTUBE_VIDEO = \"https://www.youtube.com/watch?v=cdiD-9MMpb0\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w42Jzu9Scvfd"
      },
      "source": [
        "### Defining the model.\n",
        "\n",
        "**Cost-Saving Strategy**: Adding max_token parameter to prevents the model from generating too many tokens. \n",
        "\n",
        "You can read about Prompt Parameters in my article: [\"Understanding Prompt Engineering Parameters for Enhanced Performance of LLMs\"](https://www.linkedin.com/pulse/understanding-prompt-parameters-enhanced-performance-llms-chhagani-vfv4c/?trackingId=D6N6tB2%2BRoKmgQ9galckSw%3D%3D) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "DZN09Hlccl-u"
      },
      "outputs": [],
      "source": [
        "#LangChain to use openai model\n",
        "from langchain_openai.chat_models import ChatOpenAI\n",
        "\n",
        "model = ChatOpenAI(\n",
        "\n",
        "    temperature=0,\n",
        "    max_tokens=150,\n",
        "    openai_api_key = OPENAI_API_KEY,\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q-zAv5wjdAgl",
        "outputId": "ce0ced3e-3096-47fb-e33e-b9cf02e586c2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content=\"I'm just a computer program, so I don't have feelings or emotions, but I'm here to help you with anything you need. How can I assist you today?\", response_metadata={'finish_reason': 'stop', 'logprobs': None})"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Testing the model\n",
        "model.invoke(\"How are you?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Adding a Parser to the chain\n",
        "\n",
        "The parser will only use string message in the invoke() "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Qo6XSsmDc2Id"
      },
      "outputs": [],
      "source": [
        "#Parsing only output string using langChain\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "parser = StrOutputParser()\n",
        "\n",
        "#Chained model and parser such that the result from model going into parser and then is saved in chain variable\n",
        "\n",
        "chain = model | parser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "1bzqCwlIdPvO",
        "outputId": "090d344f-d946-48c9-f4b3-69bdfe6edb69"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"I'm just a computer program, so I don't have feelings or emotions, but I'm here to help you with anything you need. How can I assist you today?\""
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Testing the chain\n",
        "chain.invoke(\"How are you?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Prompt Template"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "xnHbiUu4c_d-"
      },
      "outputs": [],
      "source": [
        "#Defining prompt template\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "template = \"\"\"\n",
        "Answer the question based on the context below. If you can't\n",
        "answer the question, reply \"I don't know\".\n",
        "\n",
        "Context: {context}\n",
        "\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "HMxRq-izddmE"
      },
      "outputs": [],
      "source": [
        "#Adding prompt to the chain\n",
        "chain = prompt | model | parser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "eHSI2iNydkz7",
        "outputId": "8594c83d-c451-4367-f3e4-31c52ad8acf1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Your name is Mahima.'"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Testing the chain\n",
        "chain.invoke({\n",
        "    \"context\": \"My name is Mahima\",\n",
        "    \"question\": \"What is my name?\"\n",
        "})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Split Text, Generate Embedding and Pinecone retrieval"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Text from YouTube video\n",
        "\n",
        "Below code uses whisper from openai to translate audio from youtube video to text and saving it in transcription.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "9yya_4mvdyrY"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import tempfile\n",
        "import whisper\n",
        "from pytube import YouTube\n",
        "\n",
        "if not os.path.exists(\"transcription.txt\"):\n",
        "    youtube = YouTube(YOUTUBE_VIDEO)\n",
        "    audio = youtube.streams.filter(only_audio=True).first()\n",
        "\n",
        "    whisper_model = whisper.load_model(\"base\")\n",
        "\n",
        "    with tempfile.TemporaryDirectory() as tmpdir:\n",
        "        file = audio.download(output_path=tmpdir)\n",
        "        transcription = whisper_model.transcribe(file, fp16=False)[\"text\"].strip()\n",
        "\n",
        "        with open(\"transcription.txt\", \"w\") as file:\n",
        "            file.write(transcription)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "CLlPFdhcrThQ",
        "outputId": "e656546d-05e1-4799-dc19-50f8514961cc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"I think it's possible that physics has exploits and we should be trying to find them. arranging some\""
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Reading first 100 char of in the txt file to verify the success of the above code\n",
        "with open(\"transcription.txt\") as file:\n",
        "    transcription = file.read()\n",
        "\n",
        "transcription[:100]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Error\n",
        "\n",
        "On invoking the chain with context as transcript, it displays error as the file exceeds the minimum token per minute value for the used model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eHmAmbusrgVI",
        "outputId": "f116660e-b705-45ad-f76c-59a35bfd5042"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-axYGPtzMjNzaBubJMB1oSL34 on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    chain.invoke({\n",
        "        \"context\": transcription,\n",
        "        \"question\": \"Is reading papers a good idea?\"\n",
        "    })\n",
        "except Exception as e:\n",
        "    print(e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "mf-bG83ErmgK"
      },
      "outputs": [],
      "source": [
        "#Loading transcript in variable text_documents\n",
        "from langchain_community.document_loaders import TextLoader\n",
        "\n",
        "loader = TextLoader(\"transcription.txt\")\n",
        "text_documents = loader.load()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Text Splitting\n",
        "\n",
        "Splitting the transcript into 1000 characters with an overlap of 20 characters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "RmpKJNaLrtIp"
      },
      "outputs": [],
      "source": [
        "\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=20)\n",
        "documents = text_splitter.split_documents(text_documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q7EcPI5pr7qs",
        "outputId": "a205404a-cde0-4052-df8e-5fee07b7cb92"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "221\n"
          ]
        }
      ],
      "source": [
        "print(len(documents))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Creating Embeddings\n",
        "\n",
        "To find revant documents from the chunks we create embeddings. You can try understanding it better by using [Cohere's Playground](https://dashboard.cohere.com/playground/embed)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "1hN3blGOsFqB"
      },
      "outputs": [],
      "source": [
        "#Embeddings from OpenAI\n",
        "from langchain_openai.embeddings import OpenAIEmbeddings\n",
        "\n",
        "embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yXH5jrwZsmRf",
        "outputId": "9c35a2f5-2a26-4d8b-f7dc-6c07ae4ddf64"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/Mahima/Library/Python/3.9/lib/python/site-packages/pydantic/_migration.py:283: UserWarning: `pydantic.error_wrappers:ValidationError` has been moved to `pydantic:ValidationError`.\n",
            "  warnings.warn(f'`{import_path}` has been moved to `{new_location}`.')\n"
          ]
        }
      ],
      "source": [
        "#Saving the embeddings to vectorestore in memory\n",
        "from langchain_community.vectorstores import DocArrayInMemorySearch\n",
        "\n",
        "vectorstore2 = DocArrayInMemorySearch.from_documents(documents, embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "MhHhSipusrNo"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'context': [Document(page_content=\"ton of text on the internet is about humans and connection and love and so on. So I think they have a very good understanding in some in some sense of of how people speak to each other about this and they're very capable of creating a lot of that kind of text. There's a lot of like sci-fi from 50s and 60s that imagined AI's in a very different way. They are calculating coal Balkan-like machines. That's not what we're getting today. We're getting pretty emotional AI's that actually are very competent and capable of generating you know possible sounding text with respect to all of these topics. Yeah I'm really hopeful about AI systems that are like companions that help you grow, develop as a human being, help you maximize long term happiness. But I'm also very worried about AI systems that figure out from the internet that humans get attracted to drama. So these would just be like shit talking AI's. Did you hear it? They'll do gossip. They'll do they'll try to plant seeds of suspicion\", metadata={'source': 'transcription.txt'}),\n",
              "  Document(page_content=\"it. It's beautiful. You're a rare human in that sense. What advice would you give to researchers trying to develop and publish idea that have a big impact in the world of AI? So maybe undergrads, maybe early graduate students. Yeah. I mean, I would say like they definitely have to be a little bit more strategic than I had to be as a PhD student because of the way AI is evolving. It's going the way of physics where, you know, in physics, you used to be able to do experiments on your bench top and everything was great and you could make progress. And now you have to work in like LHC or like CERN. And so AI is going in that direction as well. So there's certain kinds of things that's just not possible to do on the bench stop anymore. And I think that didn't used to be the case at the time. Do you still think that there's like, GAN type papers to be written? Where like, like, very simple idea that requires just one computer to illustrate a simple example. I mean, one example that's been\", metadata={'source': 'transcription.txt'}),\n",
              "  Document(page_content=\"space but in the digital space it just feels like it's going to be very tricky. Very tricky to out because it seems to be pretty low cost to fake stuff. What are you going to put an AI in jail for like trying to use a fake personhood proof? I mean okay fine you'll put a lot of AI in jail but there'll be more AI's like exponentially more. The cost of creating bought is very low. Unless there's some kind of way to track accurately like you're not allowed to create any program without showing tying yourself to that program. Like any program that runs on the internet you'll be able to trace every single human program that was involved with that program. Yeah maybe you have to start declaring when you know we have to start drawing those boundaries and keeping track of okay what are digital entities versus human entities and what is the ownership of human entities and digital entities and something like that. I don't know but I think I'm optimistic that this is possible and in some sense\", metadata={'source': 'transcription.txt'}),\n",
              "  Document(page_content=\"I think it's possible that physics has exploits and we should be trying to find them. arranging some kind of a crazy quantum mechanical system that somehow gives you buffer overflow, somehow gives you a rounding error in the floating point. Synthetic intelligences are kind of like the next stage of development. And I don't know where it leads to. Like at some point, I suspect the universe is some kind of a puzzle. These synthetic AIs will uncover that puzzle and solve it. The following is a conversation with Andre Kappathi, previously the director of AI at Tesla. And before that, at OpenAI and Stanford, he is one of the greatest scientist engineers and educators in the history of artificial intelligence. This is the Lex Friedman podcast to support it. Please check out our sponsors and now to your friends. Here's Andre Kappathi. What is a neural network? And what does it seem to do such a surprisingly good job of learning? What is a neural network? It's a mathematical abstraction of the\", metadata={'source': 'transcription.txt'})],\n",
              " 'question': 'What is the future of AI?'}"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Using retriever to retrieve data from vectorstore and passing the question through the retrieval step \n",
        "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
        "\n",
        "retriever2 = vectorstore2.as_retriever()\n",
        "setup = RunnableParallel(context=retriever2, question=RunnablePassthrough())\n",
        "setup.invoke(\"What is the future of AI?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "PFuQR1MlszE_",
        "outputId": "fc936ffc-6778-45e3-a9ab-538cf5e40d2c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'The future of AI could involve AI systems that act as companions to help individuals grow and develop, as well as AI systems that may exploit vulnerabilities in physics to advance their capabilities.'"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#chaining all together\n",
        "chain = setup | prompt | model | parser\n",
        "chain.invoke(\"What is the future of AI?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Using Pinecone\n",
        "\n",
        "Setup your [Pinecone account](https://www.pinecone.io) and create an index before running below steps. We are using pinecone to store the chunks of data along with embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "o8IVc4CkyKge"
      },
      "outputs": [],
      "source": [
        "#Insert your Pinecone API key below:\n",
        "os.environ['PINECONE_API_KEY'] = \"[ENTER YOUR PINECONE API KEY HERE]\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "O4A-HGYlySVe"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/Mahima/Library/Python/3.9/lib/python/site-packages/pinecone/data/index.py:1: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from tqdm.autonotebook import tqdm\n"
          ]
        }
      ],
      "source": [
        "#Inserting the document and embeddings in the index\n",
        "from langchain_pinecone import PineconeVectorStore\n",
        "\n",
        "index_name = \"article-rag-index\"\n",
        "\n",
        "pinecone = PineconeVectorStore.from_documents(\n",
        "    documents,embeddings,index_name=index_name\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "mA0OO04mypki"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'context': [Document(page_content=\"space but in the digital space it just feels like it's going to be very tricky. Very tricky to out because it seems to be pretty low cost to fake stuff. What are you going to put an AI in jail for like trying to use a fake personhood proof? I mean okay fine you'll put a lot of AI in jail but there'll be more AI's like exponentially more. The cost of creating bought is very low. Unless there's some kind of way to track accurately like you're not allowed to create any program without showing tying yourself to that program. Like any program that runs on the internet you'll be able to trace every single human program that was involved with that program. Yeah maybe you have to start declaring when you know we have to start drawing those boundaries and keeping track of okay what are digital entities versus human entities and what is the ownership of human entities and digital entities and something like that. I don't know but I think I'm optimistic that this is possible and in some sense\", metadata={'source': 'transcription.txt'}),\n",
              "  Document(page_content=\"space but in the digital space it just feels like it's going to be very tricky. Very tricky to out because it seems to be pretty low cost to fake stuff. What are you going to put an AI in jail for like trying to use a fake personhood proof? I mean okay fine you'll put a lot of AI in jail but there'll be more AI's like exponentially more. The cost of creating bought is very low. Unless there's some kind of way to track accurately like you're not allowed to create any program without showing tying yourself to that program. Like any program that runs on the internet you'll be able to trace every single human program that was involved with that program. Yeah maybe you have to start declaring when you know we have to start drawing those boundaries and keeping track of okay what are digital entities versus human entities and what is the ownership of human entities and digital entities and something like that. I don't know but I think I'm optimistic that this is possible and in some sense\", metadata={'source': 'transcription.txt'}),\n",
              "  Document(page_content=\"space but in the digital space it just feels like it's going to be very tricky. Very tricky to out because it seems to be pretty low cost to fake stuff. What are you going to put an AI in jail for like trying to use a fake personhood proof? I mean okay fine you'll put a lot of AI in jail but there'll be more AI's like exponentially more. The cost of creating bought is very low. Unless there's some kind of way to track accurately like you're not allowed to create any program without showing tying yourself to that program. Like any program that runs on the internet you'll be able to trace every single human program that was involved with that program. Yeah maybe you have to start declaring when you know we have to start drawing those boundaries and keeping track of okay what are digital entities versus human entities and what is the ownership of human entities and digital entities and something like that. I don't know but I think I'm optimistic that this is possible and in some sense\", metadata={'source': 'transcription.txt'}),\n",
              "  Document(page_content=\"space but in the digital space it just feels like it's going to be very tricky. Very tricky to out because it seems to be pretty low cost to fake stuff. What are you going to put an AI in jail for like trying to use a fake personhood proof? I mean okay fine you'll put a lot of AI in jail but there'll be more AI's like exponentially more. The cost of creating bought is very low. Unless there's some kind of way to track accurately like you're not allowed to create any program without showing tying yourself to that program. Like any program that runs on the internet you'll be able to trace every single human program that was involved with that program. Yeah maybe you have to start declaring when you know we have to start drawing those boundaries and keeping track of okay what are digital entities versus human entities and what is the ownership of human entities and digital entities and something like that. I don't know but I think I'm optimistic that this is possible and in some sense\", metadata={'source': 'transcription.txt'})],\n",
              " 'question': 'What is the future of AI?'}"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Retrieving data from pinecone intead of system memory\n",
        "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
        "\n",
        "pinecone_retriever = pinecone.as_retriever()\n",
        "pinecone_setup = RunnableParallel(context=pinecone_retriever, question=RunnablePassthrough())\n",
        "pinecone_setup.invoke(\"What is the future of AI?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "pAlbccbGyio-",
        "outputId": "99a7de80-932b-4e13-b166-78d9053ce891"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'The future of AI involves the challenge of distinguishing between digital entities and human entities, establishing boundaries, and tracking ownership. It may require implementing measures to accurately trace the origin of programs running on the internet.'"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Chaining pinecone setup\n",
        "chain = pinecone_setup | prompt | model | parser\n",
        "chain.invoke(\"What is the future of AI?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hTadt3dTzq0l"
      },
      "source": [
        "## Wrap retriever with LLMLingua\n",
        "\n",
        "**Cost-Saving Strategy**: LLMLingua help in prompt compression and thus reduction in input token cost. Below is an example of using [LLMLingua with LangChain](https://python.langchain.com/docs/integrations/retrievers/llmlingua). \n",
        "\n",
        "You can read more about LLMLingua in my article: [\"Cost-Saving Strategies for Large Language Models(LLMs) - Part 1\"](https://www.linkedin.com/pulse/cost-reduction-strategies-large-language-modelsllms-mahima-chhagani-7642c/?trackingId=WmayODJxThKUsQWc1jiGqw%3D%3D)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7bOpVaJnt9Ut",
        "outputId": "8dec3ba6-9b22-4e6f-950f-242475523466"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Document(page_content=\"I think it's possible that physics has exploits and we should be trying to find them arranging some kind of a crazy quantum mechanical system that somehow gives you buffer overflow, somehow gives you a rounding error in the floating point. Synthetic intelligences are kind of like the next stage of development. And I don't know where it leads to. Like at some point, I suspect the universe is some kind of a puzzle. These synthetic AIs will uncover that puzzle and solve it. The following is a conversation with Andre Kappathi, previously the director of AI at Tesla. And before that, at OpenAI and Stanford, he is one of the greatest scientist engineers and educators in the history of artificial intelligence. This is the Lex Friedman podcast to support it. Please check out our sponsors and now to your friends. Here's Andre Kappathi. What is a neural network? And what does it seem to do such a surprisingly good job of learning? What is a neural network? It's a mathematical abstraction of the\", metadata={'source': 'transcription.txt'}), Document(page_content=\"# that them...... What is a neural network? And what does it seem to do such a surprisingly good job of learning? What is a neural network? It's a mathematical abstraction of the\", metadata={'source': 'transcription.txt'})]\n"
          ]
        }
      ],
      "source": [
        "#Wrapping our base retriever with a ContextualCompressionRetriever, using LLMLinguaCompressor as a compressor\n",
        "from langchain.retrievers import ContextualCompressionRetriever\n",
        "from langchain_community.document_compressors.llmlingua_filter import LLMLinguaCompressor\n",
        "\n",
        "compressor = LLMLinguaCompressor(model_name=\"openai-community/gpt2\", device_map=\"cpu\")\n",
        "compression_retriever = ContextualCompressionRetriever(\n",
        "    base_compressor=compressor, base_retriever=pinecone_retriever #Passing pinecone_retriever in base_retriever\n",
        ")\n",
        "\n",
        "compressed_docs = compression_retriever.get_relevant_documents(\n",
        "    \"What is synthetic intelligence?\"\n",
        ")\n",
        "print(compressed_docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "-PWDmA8suZdN"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'query': 'What is synthetic intelligence?',\n",
              " 'result': 'Synthetic intelligence refers to artificial intelligence systems that are designed to mimic human-like cognitive functions and behaviors. These systems are created using algorithms and computational models to perform tasks that typically require human intelligence, such as learning, problem-solving, and decision-making.'}"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Testing compressed_retriver\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "compressed_chain = RetrievalQA.from_chain_type(llm=model, retriever=compression_retriever)\n",
        "compressed_chain.invoke(\"What is synthetic intelligence?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "WcbuKlF2BcLJ"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'context': [Document(page_content=\"space but in the digital space it just feels like it's going to be very tricky. Very tricky to out because it seems to be pretty low cost to fake stuff. What are you going to put an AI in jail for like trying to use a fake personhood proof? I mean okay fine you'll put a lot of AI in jail but there'll be more AI's like exponentially more. The cost of creating bought is very low. Unless there's some kind of way to track accurately like you're not allowed to create any program without showing tying yourself to that program. Like any program that runs on the internet you'll be able to trace every single human program that was involved with that program. Yeah maybe you have to start declaring when you know we have to start drawing those boundaries and keeping track of okay what are digital entities versus human entities and what is ownership of human entities and digital entities and something like that. I don't know but I think I'm optimistic that this is possible and in some sense\", metadata={'source': 'transcription.txt'}),\n",
              "  Document(page_content=\"<#ref>. be... is ownership of human entities and digital entities and something like that. I don't know but I think I'm optimistic that this is possible and in some sense\", metadata={'source': 'transcription.txt'})],\n",
              " 'question': 'What is the future of AI?'}"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Creating a setup for LLMLingua retriever\n",
        "LLMLingua_setup = RunnableParallel(context=compression_retriever, question=RunnablePassthrough())\n",
        "LLMLingua_setup.invoke(\"What is the future of AI?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "Jo-GPA3n6pzY"
      },
      "outputs": [],
      "source": [
        "#Chaining LLMLingua_setup instead of pinecone\n",
        "chain2 = LLMLingua_setup | prompt | model | parser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "sadPlrnZ9GK1",
        "outputId": "964f520a-f428-4c2f-f99a-d9a547422c86"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'The future of AI is uncertain and it is believed to be very tricky due to the ease of creating fake content. There is a concern about the potential for AI to be used for illegal activities, but there is also optimism that it is possible to track and regulate AI effectively.'"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Testing the chain\n",
        "chain2.invoke(\"What is the future of AI?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## LLM Caching with LangChain\n",
        "\n",
        "**Cost-Saving Strategy**: Cashing the results to avoid generating data for the same question  multiple times. This reduces the number of queries and hence save the cost."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "uHCyxbsiCZE3",
        "outputId": "b7d10a37-4e92-44e9-ea6f-01c1ab6ad49d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Synthetic intelligence is described as the next stage of development, similar to artificial intelligence but with the potential to uncover and solve the puzzle of the universe.'"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#InMemory Caching\n",
        "from langchain.globals import set_llm_cache\n",
        "from langchain.cache import InMemoryCache\n",
        "\n",
        "set_llm_cache(InMemoryCache())\n",
        "\n",
        "chain2.invoke(\"What is synthetic intelligence?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### SQLite Caching\n",
        "\n",
        "We generate a database named cache that stores the cached data. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "oWMCfu_-5ICT"
      },
      "outputs": [],
      "source": [
        "#Caching using SQLite\n",
        "from langchain.cache import SQLiteCache\n",
        "\n",
        "set_llm_cache(SQLiteCache(database_path=\".cache.db\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "OryaLmk7MLC4"
      },
      "outputs": [],
      "source": [
        "import sqlalchemy\n",
        "from sqlalchemy import create_engine, text\n",
        "\n",
        "engine = create_engine(\"sqlite:///.cache.db\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "jsrTzWIPMoLA",
        "outputId": "9f8344af-ef58-44f7-d83e-4ab7ba1729ab"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'The document is about the challenges and complexities of working with computers, setting up developer environments, hardware, environmental variables, scripts, and automation processes. It also mentions the concept of archive as a pre-print server for academic research publishing.'"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Testing\n",
        "chain2.invoke(\"What is the document about?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "3AEMSbGxNMvM",
        "outputId": "1ca1844a-409c-4cbd-fbfb-3312d7eaed1e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'The document is about the challenges and complexities of working with computers, setting up developer environments, hardware, environmental variables, scripts, and automation processes. It also mentions the concept of archive as a pre-print server for academic research publishing.'"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Testing with the same question\n",
        "chain2.invoke(\"What is the document about?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "MGsxO5T6NkAy",
        "outputId": "0af217f8-ddbe-4de1-e06e-a6e77d6cd476"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'The document is about the challenges and complexities of working with computers and setting up developer environments, as well as discussing the concept of archive as a pre-print server for academic research publishing.'"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Testing with the same question but one extra space\n",
        "chain2.invoke(\"What is the  document about?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Note**: Running the same question twice does not result in saving it in cache but when we run the same question with an extra space it creates another row in the database. To resolve this, we can use sematic caching using [redis or gptcache](https://github.com/sugarforever/LangChain-Tutorials/blob/main/LangChain_Caching.ipynb)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N6rKD6M16bWo",
        "outputId": "1a8b141d-d0f4-4dac-bfc8-c16e35a1d712"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RMKeyView(['prompt', 'llm', 'idx', 'response'])\n",
            "('[{\"lc\": 1, \"type\": \"constructor\", \"id\": [\"langchain\", \"schema\", \"messages\", \"HumanMessage\"], \"kwargs\": {\"content\": \"\\\\nAnswer the question based on t ... (1342 characters truncated) ... t about it three minutes\\', metadata={\\'source\\': \\'transcription.txt\\'})]\\\\n\\\\nQuestion: What is the document about?\\\\n\", \"additional_kwargs\": {}}}]', '{\"lc\": 1, \"type\": \"constructor\", \"id\": [\"langchain\", \"chat_models\", \"openai\", \"ChatOpenAI\"], \"kwargs\": {\"temperature\": 0.0, \"max_tokens\": 150, \"opena ... (10168 characters truncated) ... \": \"string\"}}, \"required\": [\"content\", \"tool_call_id\"]}}}}], \"edges\": [{\"source\": 0, \"target\": 1}, {\"source\": 1, \"target\": 2}]}}---[(\\'stop\\', None)]', 0, '{\"lc\": 1, \"type\": \"constructor\", \"id\": [\"langchain\", \"schema\", \"output\", \"ChatGeneration\"], \"kwargs\": {\"message\": {\"lc\": 1, \"type\": \"constructor\", \"i ... (303 characters truncated) ... as a pre-print server for academic research publishing.\", \"additional_kwargs\": {}}}, \"generation_info\": {\"finish_reason\": \"stop\", \"logprobs\": null}}}')\n",
            "('[{\"lc\": 1, \"type\": \"constructor\", \"id\": [\"langchain\", \"schema\", \"messages\", \"HumanMessage\"], \"kwargs\": {\"content\": \"\\\\nAnswer the question based on t ... (1350 characters truncated) ...  about it three minutes\\', metadata={\\'source\\': \\'transcription.txt\\'})]\\\\n\\\\nQuestion: What is the  document about?\\\\n\", \"additional_kwargs\": {}}}]', '{\"lc\": 1, \"type\": \"constructor\", \"id\": [\"langchain\", \"chat_models\", \"openai\", \"ChatOpenAI\"], \"kwargs\": {\"temperature\": 0.0, \"max_tokens\": 150, \"opena ... (10168 characters truncated) ... \": \"string\"}}, \"required\": [\"content\", \"tool_call_id\"]}}}}], \"edges\": [{\"source\": 0, \"target\": 1}, {\"source\": 1, \"target\": 2}]}}---[(\\'stop\\', None)]', 0, '{\"lc\": 1, \"type\": \"constructor\", \"id\": [\"langchain\", \"schema\", \"output\", \"ChatGeneration\"], \"kwargs\": {\"message\": {\"lc\": 1, \"type\": \"constructor\", \"i ... (241 characters truncated) ... as a pre-print server for academic research publishing.\", \"additional_kwargs\": {}}}, \"generation_info\": {\"finish_reason\": \"stop\", \"logprobs\": null}}}')\n"
          ]
        }
      ],
      "source": [
        "#Checking the cache memory\n",
        "with engine.connect() as connection:\n",
        "\n",
        "    rs = connection.exec_driver_sql('select * from full_llm_cache')\n",
        "    print(rs.keys())\n",
        "    for row in rs:\n",
        "        print(row)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tcX6d3vJNtzx"
      },
      "source": [
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
